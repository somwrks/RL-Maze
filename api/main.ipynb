{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Pool\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze:\n",
    "    #  setting up the maze\n",
    "    def __init__(self,maze,start_position,end_position):\n",
    "        self.maze=maze\n",
    "        self.start_position = start_position\n",
    "        self.end_position = end_position\n",
    "        self.maze_width = maze_layout.shape[0]\n",
    "        self.maze_height= maze_layout.shape[1]\n",
    "        \n",
    "    def show_maze(self):\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.imshow(self.maze,cmap='gray')\n",
    "        plt.text(self.start_position[0], self.start_position[1], 'S', ha='center',va='center',color='red',fontsize=20)\n",
    "        plt.text(self.end_position[0], self.end_position[1], 'E', ha='center',va='center',color='green',fontsize=20)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.show()\n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze_layout = np.array([\n",
    "    [0,1,0,0,0],\n",
    "    [0,1,1,1,0],\n",
    "    [0,0,0,1,0],\n",
    "    [1,1,0,1,1],\n",
    "    [0,0,0,0,0],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = Maze(maze_layout,(0,0),(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "maze.show_maze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [(-1,0),(1,0),(0,-1),(0,1)] # move up, down, left and right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, maze, learning_rate=0.1,discount_factor=0.9,exploration_start=1.0,exploration_end=0.01, epochs=100):\n",
    "        self.q_table = np.zeros((maze.maze_height,maze.maze_width,4))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_start = exploration_start\n",
    "        self.exploration_end = exploration_end\n",
    "        self.epochs = epochs\n",
    "    def get_exploration_rate(self,current_episode):\n",
    "        exploration_rate = self.exploration_start * (self.exploration_end/ self.exploration_start) ** (current_episode/self.epochs)\n",
    "        return exploration_rate\n",
    "    def get_action(self,state , current_episode):\n",
    "        exploration_rate = self.get_exploration_rate(current_episode)\n",
    "        \n",
    "        if np.random.rand() < exploration_rate:\n",
    "            return np.random.randint(4)\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])\n",
    "        \n",
    "    def update_q_table(self, state,action,next_state,reward):\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        current_q_value = self.q_table[state][action]\n",
    "        new_q_value = current_q_value  + self.learning_rate * (reward + self.discount_factor * self.q_table[next_state][best_next_action] - current_q_value)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_reward = 100\n",
    "wall_penalty = -10\n",
    "step_penalty = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_episode ( agent  , maze , current_episode , train = True):\n",
    "    current_state = maze.start_position\n",
    "    is_done = False\n",
    "    episode_reward = 0\n",
    "    episode_step = 0\n",
    "    path = [current_state]\n",
    "    \n",
    "    while not is_done:\n",
    "        action = agent.get_action(current_state,current_episode)\n",
    "        next_state = (current_state[0]+ actions[action][0], current_state[1] + actions[action][1])\n",
    "        if next_state[0] < 0 or next_state[0] >= maze.maze_height or next_state[1] < 0 or next_state[1] >= maze.maze_width :\n",
    "            reward = wall_penalty\n",
    "            next_state = current_state\n",
    "        elif next_state == (maze.end_position):\n",
    "            path.append(current_state)\n",
    "            reward = goal_reward\n",
    "            is_done = True\n",
    "        else:\n",
    "            path.append(current_state)\n",
    "            reward = step_penalty\n",
    "            \n",
    "        episode_reward +=reward\n",
    "        episode_step +=1\n",
    "        \n",
    "        if train == True:\n",
    "            agent.update_q_table(current_state, action,next_state,reward)\n",
    "            \n",
    "            current_state = next_state\n",
    "            \n",
    "    return episode_reward, episode_step,path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(agent,maze,num_episodes=1):\n",
    "    episode_reward, episode_step, path = finish_episode(agent,maze,num_episodes,train=False)\n",
    "    \n",
    "    print(\"Learned Path:\")\n",
    "    for row, col in path:\n",
    "        print(f\"({row},{col})-> \", end='')\n",
    "    print(\"goal!\")\n",
    "    \n",
    "    print(\"number of steps: \", episode_step)\n",
    "    print(\"Total reward: \", episode_reward)\n",
    "    \n",
    "    if plt.gcf().get_axes():\n",
    "        plt.cla()\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(maze.maze, cmpa='gray')\n",
    "    \n",
    "    plt.text(maze.start_position[0], maze.start_position[1], 'S', ha=\"center\", va='center',color='red',fontsize=20)\n",
    "    plt.text(maze.end_position[0], maze.end_position[1], 'G', ha=\"center\", va='center',color='green',fontsize=20)\n",
    "    \n",
    "    for position in path:\n",
    "        plt.text(position[0], position[1],\"#\",va=\"center\",color=\"blue\",fontsize=20)\n",
    "    \n",
    "    plt.xticks([],plt.yticks([]))\n",
    "    plt.grid(color='black',linewidth=2)\n",
    "    plt.show()\n",
    "    return episode_step, episode_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent,maze,epochs=100):\n",
    "    episode_rewards = []\n",
    "    episode_steps = []\n",
    "    \n",
    "    for episode in range(epochs):\n",
    "        episode_reward,episode_step,path = finish_episode(agent,maze,episode,train = True)\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_steps.append(episode_step)\n",
    "        \n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(episode_rewards)\n",
    "    plt.xlabel(\"episode\")\n",
    "    plt.ylabel(\"cumulative reward\")\n",
    "    plt.title(\"reward per episode\")\n",
    "    \n",
    "    average_reward = sum(episode_rewards) / len(episode_rewards)\n",
    "    print(f\"the average reward is : {average_reward}\")\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(episode_steps)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Steps taken\")\n",
    "    plt.ylim(0,100)\n",
    "    plt.title(\"Steps per episode\")\n",
    "    \n",
    "    average_steps = sum(episode_steps) / len(episode_steps)\n",
    "    print(f\"The average steps is : {average_steps}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(maze)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 1: Average Reward = -93086.92, Average Steps = 13716.84\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def train_worker(args):\n",
    "    agent, maze, epochs = args\n",
    "    episode_rewards = []\n",
    "    episode_steps = []\n",
    "    \n",
    "    for episode in range(epochs):\n",
    "        episode_reward, episode_step, _ = finish_episode(agent, maze, episode, train=True)\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_steps.append(episode_step)\n",
    "    \n",
    "    return episode_rewards, episode_steps\n",
    "\n",
    "def parallel_train_agent(num_agents, epochs, maze):\n",
    "    agents = [Agent(maze) for _ in range(num_agents)]\n",
    "    arguments = [(agent, maze, epochs) for agent in agents]\n",
    "    \n",
    "    with Pool(processes=num_agents) as pool:\n",
    "        results = pool.map(train_worker, arguments)\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        episode_rewards, episode_steps = result\n",
    "        print(f\"Agent {i+1}: Average Reward = {sum(episode_rewards)/len(episode_rewards)}, Average Steps = {sum(episode_steps)/len(episode_steps)}\")\n",
    "\n",
    "# Example usage\n",
    "parallel_train_agent(num_agents=1, epochs=25, maze=maze)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[173], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[43mparallel_test_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_agents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaze\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaze\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[173], line 23\u001b[0m, in \u001b[0;36mparallel_test_agent\u001b[0;34m(num_agents, epochs, maze)\u001b[0m\n\u001b[1;32m     20\u001b[0m arguments \u001b[38;5;241m=\u001b[39m [(agent, maze, epochs) \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m agents]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool(processes\u001b[38;5;241m=\u001b[39mnum_agents) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[0;32m---> 23\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_worker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marguments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(results):\n\u001b[1;32m     26\u001b[0m     total_steps, total_reward, episode_rewards, episode_steps \u001b[38;5;241m=\u001b[39m result\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def test_worker(args):\n",
    "    agent, maze, epochs = args\n",
    "    total_steps = 0\n",
    "    total_reward = 0\n",
    "    episode_rewards = []\n",
    "    episode_steps = []\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        # Correctly call test_agent once per epoch\n",
    "        episode_steps, episode_reward = test_agent(agent, maze, num_episodes=1)\n",
    "        total_steps += episode_steps\n",
    "        total_reward += episode_reward\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_steps.append(episode_steps)\n",
    "    \n",
    "    return total_steps, total_reward, episode_rewards, episode_steps\n",
    "\n",
    "def parallel_test_agent(num_agents, epochs, maze):\n",
    "    agents = [Agent(maze) for _ in range(num_agents)]\n",
    "    arguments = [(agent, maze, epochs) for agent in agents]\n",
    "    \n",
    "    with Pool(processes=num_agents) as pool:\n",
    "        results = pool.map(test_worker, arguments)\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        total_steps, total_reward, episode_rewards, episode_steps = result\n",
    "        print(f\"Agent {i+1}: Total Steps = {total_steps}, Total Reward = {total_reward}\")\n",
    "        \n",
    "        # Plotting\n",
    "        plt.figure(i*2)\n",
    "        plt.plot(range(len(episode_rewards)), episode_rewards)\n",
    "        plt.title(f'Agent {i+1} Episode Rewards')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Reward')\n",
    "        \n",
    "        plt.figure(i*2+1)\n",
    "        plt.plot(range(len(episode_steps)), episode_steps)\n",
    "        plt.title(f'Agent {i+1} Episode Steps')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Steps')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "parallel_test_agent(num_agents=1, epochs=25, maze=maze)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
